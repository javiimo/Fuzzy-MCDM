# Fuzzy Multi-Criteria Decision Analysis for Maintenance Scheduling

![Python Version](https://img.shields.io/badge/python-3.9+-blue.svg)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the full implementation of a fuzzy multi-criteria decision-making (MCDM) model designed to analyze and rank complex maintenance schedules. The model was developed as a case study for the **ROADEF/EURO Challenge 2020**, which focused on a real-world maintenance planning problem from the French transmission system operator, RTE (Réseau de Transport d'Électricité).

The core of this project is to provide a transparent and justifiable framework for a decision-maker to select the best maintenance schedule from a pool of highly competitive alternatives, considering a wide range of conflicting and imprecise criteria.

> 🔗 **This work is part of a broader academic work, which can be found here:** [**TFG-Math-UV/Fuzzy-MCDM**](https://github.com/javiimo/TFG-Math-UV)

---

## 📋 Table of Contents

- [Fuzzy Multi-Criteria Decision Analysis for Maintenance Scheduling](#fuzzy-multi-criteria-decision-analysis-for-maintenance-scheduling)
  - [📋 Table of Contents](#-table-of-contents)
  - [🚀 Project Overview](#-project-overview)
  - [CHALLENGE Background: The ROADEF 2020 Challenge](#challenge-background-the-roadef-2020-challenge)
  - [🛠️ Methodology](#️-methodology)
    - [1. Alternative Generation](#1-alternative-generation)
    - [2. Attributes](#2-attributes)
      - [A. Retrieving Physical Locations (MDS)](#a-retrieving-physical-locations-mds)
      - [B. Defining Crisp Attributes](#b-defining-crisp-attributes)
      - [C. Defining Fuzzy Attributes](#c-defining-fuzzy-attributes)
    - [3. The Decision Matrix](#3-the-decision-matrix)
    - [4. Aggregation \& Ranking Model](#4-aggregation--ranking-model)
      - [Level 1: Intra-Block Aggregation (Epsilon-Lexicographic)](#level-1-intra-block-aggregation-epsilon-lexicographic)
      - [Level 2: Inter-Block Aggregation (Ordered Weighted Averaging - OWA)](#level-2-inter-block-aggregation-ordered-weighted-averaging---owa)
  - [📂 Repository Structure](#-repository-structure)
  - [⚙️ How to Run](#️-how-to-run)
    - [Prerequisites](#prerequisites)
    - [Data Acquisition](#data-acquisition)
    - [Execution Pipeline](#execution-pipeline)
  - [🔗 Dependencies \& External Assets](#-dependencies--external-assets)
    - [Key Python Libraries](#key-python-libraries)
    - [External Links](#external-links)
  - [📜 License](#-license)

---

## 🔍 Project Overview

Preventive maintenance on a power grid is a double-edged sword: while essential for long-term reliability, taking a power line out of service temporarily weakens the network. This project tackles the complex optimization problem of scheduling these maintenance tasks over a year.

Instead of relying on a single, aggregated cost function, this model evaluates schedules against a rich set of criteria, including:
-   **Temporal Distribution:** How uniformly are tasks spread out?
-   **Risk & Size:** Are high-risk or large-workload tasks clustered together?
-   **Spatial Proximity:** Are interventions in the same geographical area scheduled concurrently?
-   **Environmental Impact:** How close are interventions to sensitive areas like national parks?

To handle the inherent imprecision and qualitative nature of these criteria (e.g., what does "close" or "high-risk" mean?), this project employs **Fuzzy Logic**. Crisp data is transformed into fuzzy sets, and a two-level aggregation model allows a decision-maker to rank alternatives based on their priorities and risk attitude.

##  CHALLENGE Background: The ROADEF 2020 Challenge

The problem is based on the **ROADEF/EURO Challenge 2020**, a prestigious competition that bridges academia and industry. The 2020 edition, in collaboration with RTE, focused on creating a robust annual maintenance schedule.

-   **Objective:** Minimize a composite risk objective function while respecting complex operational constraints (resource availability, safety exclusions).
-   **Core Data:** RTE provided pre-calculated financial risk data for each potential maintenance task across thousands of future scenarios.
-   **Alternatives:** The schedules analyzed in this project were generated by the algorithms of the top-performing teams from the competition.

## 🛠️ Methodology

The decision-making framework is built in four main stages:

### 1. Alternative Generation

The pool of alternatives consists of **29 distinct maintenance schedules**. These were generated by running the algorithms from three of the top-ranked teams in the ROADEF challenge on the difficult `X12` instance. Each algorithm was run with different time limits (180s to 900s) and random seeds to produce a diverse set of high-quality solutions.

> The `Decision Matrix/Alternatives/Solvers/execute.py` script was used to run the compiled solvers and generate the raw solution files located in `Decision Matrix/Alternatives/X12/`.

### 2. Attributes

To evaluate the alternatives, we first needed to define a set of meaningful attributes. This involved a significant data engineering effort.

#### A. Retrieving Physical Locations (MDS)

The physical locations of interventions were not public. To infer plausible relative locations, a key assumption was made:

> **Assumption:** Interventions that are physically close have a similar risk profile over time.

Based on this, we:
1.  Calculated a similarity matrix between all pairs of interventions by correlating their risk variations over the entire time horizon.
2.  Transformed this similarity matrix into a dissimilarity matrix. Negative or zero correlations were treated as "missing data."
3.  Used **Non-metric Multi-Dimensional Scaling (MDS)** to find a 2D embedding of the interventions that respects the rank-ordering of the dissimilarities. The methodology is based on the **SMACOF algorithm**, which is robust to missing data.
4.  Stretched and translated this 2D point cloud to align with a map of France, enabling spatial analysis.

This entire process is implemented in `point_gen.py`.

#### B. Defining Crisp Attributes

-   **Highest Concurrency:** The maximum number of interventions running simultaneously on any given day. A lower value is better.
-   **Seasonality:** The proportion of interventions active in each season (Winter, Summer, Inter-season). These are trade-off criteria.

#### C. Defining Fuzzy Attributes

Four fuzzy attributes were designed to capture the temporal uniformity of different types of concurrent tasks. The process for each is:

1.  **Fuzzification:** Raw intervention data (e.g., mean size, mean risk, distance) is transformed into fuzzy sets.
    -   For **Size** and **Risk**, **Fuzzy C-Means Clustering** (`scikit-fuzzy`) is used to partition interventions into 5 clusters (e.g., from `small` to `large`). Each intervention receives a membership degree to each cluster.
    -   For **Closeness** and **Environmental Impact**, a **linguistic variable** with 5 terms (from `close` to `far`) is defined using trapezoidal and triangular membership functions.
2.  **Daily Mass Calculation:** For each fuzzy term (e.g., "large size" or "close to park"), a "daily mass" is calculated by summing the membership degrees of all active interventions on each day of the year. This yields a time series for each fuzzy property.
3.  **Uniformity Scoring:** The uniformity of each daily mass distribution is quantified using **Shannon's Entropy**. A higher entropy score corresponds to a more uniform (and desirable) distribution. The final score is normalized to [0, 1].

This logic is implemented across `fuzzy_var.py`, `map.py`, and the `Solution` class in `my_data_structs.py`.

### 3. The Decision Matrix

The attributes for all 29 alternatives are compiled into a single decision matrix. This matrix is the primary input for the final aggregation step.

-   **Generation:** `DM_Matrix.py` reads the raw solution files, computes all crisp and fuzzy attributes, and saves the result.
-   **Output:** The final matrix is available as `decision_matrix_expanded.csv` and a formatted `decision_matrix_expanded.md`. A LaTeX version can be generated using `Dmmatrix2latex.py`.

### 4. Aggregation & Ranking Model

A two-level aggregation model is used to compute a final score for each alternative:

#### Level 1: Intra-Block Aggregation (Epsilon-Lexicographic)

The fuzzy attributes are grouped into four conceptual blocks (Size, Risk, Closeness, Env. Impact). Within each block, the criteria are strictly ordered by importance (e.g., `Risk_high` > `Risk_mid_high` > ...).

An **epsilon-lexicographic** method ensures this strict, non-compensatory hierarchy. A lower-priority criterion is *only* used as a tie-breaker and can never compensate for a poor score on a higher-priority one.

#### Level 2: Inter-Block Aggregation (Ordered Weighted Averaging - OWA)

The six resulting concept scores (Concurrency, Size, Risk, etc.) are aggregated using an **OWA operator**. This allows the model to reflect the decision-maker's attitude:
-   **Pessimistic (orness ≈ 0):** The final score is dominated by the worst-performing attribute (a `min` operation).
-   **Optimistic (orness ≈ 1):** The score is driven by the best-performing attribute (a `max` operation).
-   **Neutral (orness = 0.5):** A simple average.

This aggregation logic is implemented in `epsilon_decision.py`, which also generates the final rankings and analysis plots.


---

## 📂 Repository Structure

The repository is organized into a main directory containing scripts and subdirectories for data and generated files.

| File / Directory                               | Description                                                                                             |
| ---------------------------------------------- | ------------------------------------------------------------------------------------------------------- |
| **Core Logic & Analysis**                      |                                                                                                         |
| `my_data_structs.py`                           | Defines the core data classes (`Instance`, `Intervention`, `Solution`) and methods for computing attributes like concurrency and entropy. |
| `fuzzy_var.py`                                 | Implements fuzzy logic components: t-norms, t-conorms, and membership functions (`triangular`, `trapezoidal`). |
| `epsilon_decision.py`                          | The final decision analysis script. Implements the epsilon-lexicographic and OWA aggregation, and generates the final rankings and plots. |
| `loowa_decision.py`                            | An alternative implementation using Lexicographic Ordinal OWA (LOOWA) instead of the epsilon method.     |
| **Data Generation & Processing**               |                                                                                                         |
| `point_gen.py`                                 | Infers intervention locations using risk correlation and non-metric MDS. Generates `points_*.npy` files. |
| `map.py`                                       | Computes fuzzy distance matrices between interventions and between interventions and parks.              |
| `DM_Matrix.py`                                 | Reads raw solution files, computes all attributes, and builds the final decision matrix CSV.           |
| `Dmmatrix2latex.py`                            | A utility script to convert the final decision matrix CSV into a styled LaTeX table.                    |
| **Generated Data & Outputs**                   |                                                                                                         |
| `decision_matrix_expanded.csv` / `.md`         | The final decision matrix containing all attributes for all alternatives.                               |
| `points_*.npy` / `points_keys_*.npy`            | The 2D coordinates and corresponding keys for interventions, as generated by `point_gen.py`.          |
| `Decision Matrix/`                             | Contains the raw solution files and original instance data.                                             |
| `geojsons_nat_parks/`                          | Contains GeoJSON data for French national parks.                                                        |
| **Miscellaneous**                              |                                                                                                         |
| `test.ipynb`                                   | A Jupyter notebook for testing and experimentation.                                                     |

---

## ⚙️ How to Run

### Prerequisites

-   Python 3.9+
-   Required Python libraries. You can install them via pip:
    ```bash
    pip install pandas numpy matplotlib geopandas scikit-learn scikit-fuzzy tqdm shapely pyproj
    ```

### Data Acquisition

The primary instance file (`X_12.json`) used for this analysis is **313 MB** and is not included in this repository. You must download it manually.

1.  Go to the [ROADEF 2020 Challenge Instances page](https://roadef.org/challenge/2020/en/instances.php).
2.  Download the **Set X** instances (`instances_X.tar.gz`).
3.  Extract the archive and place `X_12.json` into the `Decision Matrix/Difficult Instances/` directory.

### Execution Pipeline

The scripts should be run in the following order to reproduce the analysis:

1.  **Generate Intervention Locations:**
    This script performs the MDS analysis to create the point cloud representing intervention locations.
    ```bash
    python point_gen.py
    ```
    This will generate new `points_*.npy` and `points_keys_*.npy` files. Update the filenames in `DM_Matrix.py` to use the newly generated files.

2.  **Build the Decision Matrix:**
    This script processes all raw solution files and computes the full decision matrix.
    ```bash
    python DM_Matrix.py
    ```
    This will create `decision_matrix_expanded.csv` and `decision_matrix_expanded.md`.

3.  **Run the Final Analysis:**
    This script reads the decision matrix, applies the aggregation model, and generates the final analysis plots and rankings.
    ```bash
    python epsilon_decision.py
    ```

---

## 🔗 Dependencies & External Assets

### Key Python Libraries

| Library         | Purpose                                                              |
| --------------- | -------------------------------------------------------------------- |
| `pandas`        | Data manipulation and storage (DataFrames).                          |
| `numpy`         | Numerical operations and matrix computations.                        |
| `matplotlib`    | Data visualization and plotting.                                     |
| `geopandas`     | Handling of geospatial data (e.g., national park polygons).          |
| `smacof` (R package via rpy2) | Used for non-metric Multi-Dimensional Scaling as implemented in R. |
| `scikit-fuzzy`  | Used for the Fuzzy C-Means clustering algorithm.                     |
| `shapely`       | Manipulation and analysis of planar geometric objects.               |
| `pyproj`        | Projections and coordinate system transformations.                   |
| `tqdm`          | Progress bars for long-running computations.                         |

### External Links

-   **Complete Academic Work:** [A Fuzzy Multi-Criteria Decision-Making Analysis](https://github.com/javiimo/TFG-Math-UV)
-   **Challenge Instances:** [ROADEF/EURO 2020 Instances](https://roadef.org/challenge/2020/en/instances.php)
-   **Team 1 Solver:** [mbmv7/rc](https://github.com/mbmv7/rc)
-   **Team 3 Solver:** [FranciscoParrenoTorres/Roadef2020](https://github.com/FranciscoParrenoTorres/Roadef2020)
-   **Team 5 Solver:** [RWTH Aachen sciebo](https://rwth-aachen.sciebo.de/s/BoMYqRmy7GbwlIm)
**French map data:** [French national/regional parks polygons](https://www.data.gouv.fr/fr/datasets/parcs-naturels-regionaux-pnr-france-metropolitaine)

---

## 📜 License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.